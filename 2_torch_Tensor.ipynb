{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_torch_Tensor.ipynb","provenance":[],"collapsed_sections":["iSFNUuhurvS-","tBe2jl7csCHN","-Hkt4gpUt5pl"],"toc_visible":true,"authorship_tag":"ABX9TyMH01HQMlpVjdnSCPj9c/Hl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NBfPLpGUCuw6"},"source":["# torch.Tensor\n","\n","- torch.Tensor는 Tensor객체의 메소드\n","- 메소드 말고 속성도 있음(dtype, device, layout, memory_format)\n","- torch와 거의 유사\n","- torch와 비교해 In-place연산자(_)가 많음"]},{"cell_type":"code","metadata":{"id":"k2dBzsGRrZVB"},"source":["import numpy as np\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15Z9aqdwq4xw"},"source":["## Tensor class reference"]},{"cell_type":"markdown","metadata":{"id":"iSFNUuhurvS-"},"source":["### Tensor.T (attribute)\n","- transpose\n","- memory 공유"]},{"cell_type":"code","metadata":{"id":"PEaTtWyxCMyK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632518884950,"user_tz":-540,"elapsed":331,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"88224f3d-ea82-423b-cfbb-17df74fb5dd9"},"source":["x = torch.rand(1, 2, 3, 4)\n","\n","y = x.T # (4, 3, 2, 1)\n","\n","y[2][1][0][0] = 100\n","print(y[2][1][0][0] == x[0][0][1][2])\n","\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(True)\n","torch.Size([4, 3, 2, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"tBe2jl7csCHN"},"source":["### Tensor.t()\n","- Tensor.t()\n","- 2차원 이하의 텐서만 지원\n","- 0차원과 1차원을 바꾼다\n","- memory 공유"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1riuDYhUr80d","executionInfo":{"status":"ok","timestamp":1632518997796,"user_tz":-540,"elapsed":411,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"56c52cc2-f151-4dcd-be1f-af352447cd5a"},"source":["x = torch.rand(2, 5)\n","\n","y = x.t()\n","y[0][1] = 100\n","print(y[0][1] == x[1][0])\n","print(y.shape)\n","\n","y.t_() # in-place version\n","\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(True)\n","torch.Size([5, 2])\n","torch.Size([2, 5])\n"]}]},{"cell_type":"code","metadata":{"id":"w9vlwQ8BsdAn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Hkt4gpUt5pl"},"source":["### Tensor.new_tensor(), new_full(), new_empty(), new_ones() new_zeros()\n","- 새로운 텐서 반환\n","- 이해가 안가는 점은 Tensor.new_tensor(data) 이런 식이라는 점이다\n","- Tensor가 어떤 값인지 전혀 중요하지 않다, data를 갖는 텐서를 만든다\n","    - 그러면 torch.new_tensor(data) 이런 식이 더 낫지 않나? 어떤 목적으로 이렇게 만들어진걸까?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMBXSbLcuNeM","executionInfo":{"status":"ok","timestamp":1632519561818,"user_tz":-540,"elapsed":309,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"a9ec70be-af99-453e-ee1e-82bc3a981fa4"},"source":["x = torch.rand(5, 2)\n","\n","x_new = x.new_tensor(torch.rand(4, 4))\n","\n","x_new"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8645, 0.7358, 0.7237, 0.1308],\n","        [0.4840, 0.1035, 0.9089, 0.3443],\n","        [0.1960, 0.5444, 0.7848, 0.1441],\n","        [0.3978, 0.4933, 0.5694, 0.4149]])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"2cj3Mbe22Lrp"},"source":["### Tensor.is_cuda\n","- Is True if the Tensor is stored on the GPU, False otherwise."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfhD-Tl5g64i","executionInfo":{"status":"ok","timestamp":1632549212419,"user_tz":-540,"elapsed":317,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"82be41dd-5a65-4ecf-d31d-7cba64ce87b3"},"source":["x = torch.randint(10, (2, 3))\n","x.is_cuda"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"eeKt108n2LpO"},"source":["### Tensor.device\n","- Is the torch.device where this Tensor is."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HCj1yz-Ignk-","executionInfo":{"status":"ok","timestamp":1632549169836,"user_tz":-540,"elapsed":293,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"ad6ba2f8-533f-4b22-f147-9ce569bd0d72"},"source":["x = torch.randint(10, (2, 2))\n","x.device"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"V7m5Npbu30mY"},"source":["### Tensor.cpu()\n","- Returns a copy of this object in CPU memory.\n","- If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned"]},{"cell_type":"markdown","metadata":{"id":"HTbfLX6f3357"},"source":["### Tensor.cuda()\n","- Returns a copy of this object in CUDA memory\n","- If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.\n","- parameter: device (torch.device) – The destination GPU device. Defaults to the current CUDA device."]},{"cell_type":"markdown","metadata":{"id":"9vPJ4MQi5bJQ"},"source":["### Tensor.get_device()\n","- For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. \n","- For CPU tensors, an error is thrown"]},{"cell_type":"markdown","metadata":{"id":"68H_zzOi3Lip"},"source":["### Tensor.backward()\n","- Computes the gradient of current tensor w.r.t.(with respect to) graph leaves.\n","- This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. \n","- If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see [Stream semantics of backward passes](https://pytorch.org/docs/stable/notes/cuda.html#bwd-cuda-stream-semantics)."]},{"cell_type":"markdown","metadata":{"id":"6myCviHb2LjB"},"source":["### Tensor.grad()\n","- This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. \n","- The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it.\n","_  loss.backward()를 호출하면 autograd는 gradient를 계산하고 이를 텐서의 .grad 속성(attribute)에 저장합니다"]},{"cell_type":"markdown","metadata":{"id":"kbbDZbSJ4IpX"},"source":["### Tensor.detach()\n","- Returns a new Tensor, detached from the current graph\n","- The result will never require gradient\n","- Returned Tensor shares the same storage with the original one."]},{"cell_type":"markdown","metadata":{"id":"SE4grwZs6M3U"},"source":["### Tensor.numpy()\n","- Returns self tensor as a NumPy ndarray. This tensor and the returned ndarray share the same underlying storage.\n","- Changes to self tensor will be reflected in the ndarray and vice versa."]},{"cell_type":"markdown","metadata":{"id":"AGN15FJHaT7K"},"source":["### Tensor.to()\n","- Performs Tensor dtype and/or device conversion\n","- Returns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. \n","- dtype이나 device를 바꾸게 되면 memory는 share될 수 없을 것으로 예상"]},{"cell_type":"code","metadata":{"id":"NXA9FF-veFmj","executionInfo":{"status":"ok","timestamp":1632548475610,"user_tz":-540,"elapsed":410,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}}},"source":["# >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n","# >>> tensor.to(torch.float64)\n","# tensor([[-0.5044,  0.0005],\n","#         [ 0.3310, -0.0584]], dtype=torch.float64)\n","\n","# >>> cuda0 = torch.device('cuda:0')\n","# >>> tensor.to(cuda0)\n","# tensor([[-0.5044,  0.0005],\n","#         [ 0.3310, -0.0584]], device='cuda:0')\n","\n","# >>> tensor.to(cuda0, dtype=torch.float64)\n","# tensor([[-0.5044,  0.0005],\n","#         [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n","\n","# >>> other = torch.randn((), dtype=torch.float64, device=cuda0)\n","# >>> tensor.to(other, non_blocking=True)\n","# tensor([[-0.5044,  0.0005],\n","#         [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cBfWKGin9Nra"},"source":["### Tensor.tolist()\n","- Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). \n","- Tensors are automatically moved to the CPU first if necessary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGrVf0xCfZZA","executionInfo":{"status":"ok","timestamp":1632548903943,"user_tz":-540,"elapsed":437,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"6bf6b5b6-502a-471c-a90a-5419b60bf68d"},"source":["x1 = torch.rand(2, 2)\n","x2 = torch.rand(1, 1)\n","\n","y1 = x1.tolist()\n","y2 = x2.tolist()\n","y3 = x2.item()\n","\n","print(y1, y2, y3)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.7422759532928467, 0.22756850719451904], [0.4219493865966797, 0.029946625232696533]] [[0.2920449376106262]] 0.2920449376106262\n"]}]},{"cell_type":"markdown","metadata":{"id":"tyYpyjZ4fRYc"},"source":["### Tensor.item()\n","- Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist()."]},{"cell_type":"markdown","metadata":{"id":"LoAOZBUD2LgQ"},"source":["### Tensor.abs(), abs_(), real(), imag()"]},{"cell_type":"markdown","metadata":{"id":"XOz8FbJ72Ldh"},"source":["### Tensor.apply_()"]},{"cell_type":"markdown","metadata":{"id":"KnVC7DSF4XgU"},"source":["### Tensor.expand()\n","- Tensor.expand(*sizes)\n","- Returns a new view of the self tensor with singleton dimensions expanded to a larger size.\n","\n","- Passing -1 as the size for a dimension means not changing the size of that dimension.\n","- More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations may result in incorrect behavior. If you need to write to the tensors, please clone them first."]},{"cell_type":"markdown","metadata":{"id":"WJcRKMOiWghj"},"source":["### Tensor.repeat()\n","- Tensor.repeat(*sizes)\n","- Repeats this tensor along the specified dimensions.\n","- Unlike expand(), this function copies the tensor’s data."]},{"cell_type":"markdown","metadata":{"id":"6On9sP652LTC"},"source":["### Tensor.argmax(), argmin(), argsort()"]},{"cell_type":"markdown","metadata":{"id":"0RVLgDVR2LQO"},"source":["### Tensor.ceil(), floor()"]},{"cell_type":"markdown","metadata":{"id":"dlKdekvJ2LNg"},"source":["### Tensor.clone()"]},{"cell_type":"markdown","metadata":{"id":"rwFhvSYv3pn7"},"source":["### Tensor.copy_()"]},{"cell_type":"markdown","metadata":{"id":"6sjeeGMyYJkJ"},"source":["### Tensor.view()"]},{"cell_type":"markdown","metadata":{"id":"5gFLh8lu2LK5"},"source":["### Tensor.contiguous()\n","- Returns a contiguous in memory tensor containing the same data as self tensor.\n","- If self tensor is already in the specified memory format, this function returns the self tensor."]},{"cell_type":"markdown","metadata":{"id":"QUaDpASv5pfv"},"source":["### Tensor.is_contiguous()\n","- Returns True if self tensor is contiguous in memory in the order specified by memory format."]},{"cell_type":"markdown","metadata":{"id":"YpPP6Jvt5xTs"},"source":["### Tensor.is_leaf()\n","- All Tensors that have requires_grad which is False will be leaf Tensors by convention.\n","\n","- For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.\n","\n","- Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad().\n","\n"]},{"cell_type":"code","metadata":{"id":"AcyjVoukeU58","executionInfo":{"status":"ok","timestamp":1632548538365,"user_tz":-540,"elapsed":351,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}}},"source":["# >>> a = torch.rand(10, requires_grad=True)\n","# >>> a.is_leaf\n","# True\n","\n","# >>> b = torch.rand(10, requires_grad=True).cuda()\n","# >>> b.is_leaf\n","# False\n","# # b was created by the operation that cast a cpu Tensor into a cuda Tensor\n","\n","# >>> c = torch.rand(10, requires_grad=True) + 2\n","# >>> c.is_leaf\n","# False\n","# # c was created by the addition operation\n","\n","# >>> d = torch.rand(10).cuda()\n","# >>> d.is_leaf\n","# True\n","# # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n","\n","# >>> e = torch.rand(10).cuda().requires_grad_()\n","# >>> e.is_leaf\n","# True\n","# # e requires gradients and has no operations creating it\n","\n","# >>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n","# >>> f.is_leaf\n","# True\n","# # f requires grad, has no operation creating it"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WzvXrUb85xLn"},"source":["### Tensor.is_shared()\n","- checks if tensor is in shared memory.\n","- This is always True for CUDA tensors."]},{"cell_type":"markdown","metadata":{"id":"6l_2Qn532K_D"},"source":["### Tensor.flatten()\n","- torch.flatten(input, start_dim=0, end_dim=-1)\n","- Flattens input by reshaping it into a one-dimensional tensor.\n","- Numpy의 flatten은 항상 copy, tensor는 view될 수도, copy될 수도 -> tensor.reshape과 유사"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rRhJurVZluw","executionInfo":{"status":"ok","timestamp":1632547350616,"user_tz":-540,"elapsed":366,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"f32ea3b3-856b-4d31-d7ad-d5d5c6bd348a"},"source":["x = torch.rand(2, 3, 4)\n","\n","y1 = x.flatten()\n","\n","y2 = x.flatten(start_dim=1)\n","\n","y3 = x.flatten(start_dim=2)\n","\n","y4 = x.flatten(end_dim=1)\n","\n","print(y1.shape, y2.shape, y3.shape, y4.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([24]) torch.Size([2, 12]) torch.Size([2, 3, 4]) torch.Size([6, 4])\n"]}]},{"cell_type":"markdown","metadata":{"id":"oZZsuAcH2K73"},"source":["### Tensor.float(), bool(), char()"]},{"cell_type":"markdown","metadata":{"id":"C3al21Hj2K4m"},"source":["### Tensor.register_hook()"]},{"cell_type":"markdown","metadata":{"id":"HKGD7EaA2KNP"},"source":["### Tensor.requires_grad(), requires_grad_()\n","- Is True if gradients need to be computed for this Tensor, False otherwise.\n","- The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details."]},{"cell_type":"markdown","metadata":{"id":"GBmgDCnC8SpY"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"4Eapumum8Slp"},"source":["### Tensor.size()"]},{"cell_type":"markdown","metadata":{"id":"Sz1-el0K8Sh7"},"source":["### Tensor.to_sparse()"]},{"cell_type":"markdown","metadata":{"id":"jK1SKiQ28Sef"},"source":["### Tensor.type(), type_as()"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lu0oXUgp92bF","executionInfo":{"status":"ok","timestamp":1632540047287,"user_tz":-540,"elapsed":10,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"50a5a17e-acad-498f-86f2-7c0e27c5024c"},"source":["import torch\n","x = torch.rand(1, 2)\n","print(x.type())\n","print(type(x))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.FloatTensor\n","<class 'torch.Tensor'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"-z3-UhNM8SVs"},"source":["### Tensor.unfold()"]},{"cell_type":"markdown","metadata":{"id":"TT6XVS7L-qBf"},"source":["### Tensor.unique()"]},{"cell_type":"code","metadata":{"id":"sf1VHYf7YCsu"},"source":[""],"execution_count":null,"outputs":[]}]}